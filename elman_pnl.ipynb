{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Qualitatively replicate: \n",
    "\n",
    "[1] Elman, J. L. (1990). Finding structure in time. \n",
    "Cognitive Science, 14(2), 179–211. \n",
    "https://doi.org/10.1016/0364-0213(90)90002-E\n",
    "\n",
    "[2] Saffran, J. R., Aslin, R. N., & Newport, E. L. (1996). Statistical learning by 8-month-old infants. \n",
    "Science, 274(5294), 1926–1928. \n",
    "https://doi.org/10.1126/science.274.5294.1926\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for google colab\n",
    "!pip install psyneulink==0.5.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import psyneulink as pnl\n",
    "from psyneulink import AutodiffComposition\n",
    "import os \n",
    "import time \n",
    "import string \n",
    "import warnings\n",
    "import itertools\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(pnl.__version__)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style='white', context='poster', font_scale=.8, rc={\"lines.linewidth\": 2})\n",
    "\n",
    "seed_val = 0\n",
    "np.random.seed(seed_val)\n",
    "\n",
    "%matplotlib inline \n",
    "%autosave 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_letters = string.ascii_lowercase\n",
    "\n",
    "# define all vocabs\n",
    "chunk_size = 4\n",
    "all_vocabs = [\n",
    "    all_letters[i:i + chunk_size]\n",
    "    for i in range(0, len(all_letters), chunk_size) \n",
    "]\n",
    "print(f'All vocabs:\\n{all_vocabs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen seqs, given some vocabs\n",
    "def gen_story(all_vocabs, seq_len): \n",
    "    n_vocabs = len(all_vocabs)\n",
    "    seq_ids = np.random.randint(n_vocabs, size=seq_len)\n",
    "    seq = [all_vocabs[i] for i in seq_ids]\n",
    "    # integer representation\n",
    "    seq_int = [\n",
    "        [all_letters.index(letter) for letter in vocab]\n",
    "        for vocab in seq\n",
    "    ]\n",
    "    return seq, seq_int\n",
    "\n",
    "seq_len = 12\n",
    "seq, seq_int = gen_story(all_vocabs, seq_len)\n",
    "print(f'Here\\'s a \"story\":\\n{seq}')\n",
    "print(f'The corresponding int representation:\\n{seq_int}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize the input \n",
    "def onehot_transform(seq_int_): \n",
    "    # get the unit of representation\n",
    "    n_letters = len(all_letters)\n",
    "    all_letters_ohe_template = np.reshape(np.arange(n_letters),newshape=(-1,1))\n",
    "    # init one hot encoder\n",
    "    ohe = OneHotEncoder(sparse=False)\n",
    "    ohe.fit(all_letters_ohe_template)\n",
    "    # reformat the sequence\n",
    "    seq_int_ = [np.reshape(vocab, newshape=(-1,1)) for vocab in seq_int_]\n",
    "    # transform to one hot \n",
    "    seq_ohe = [ohe.transform(vocab) for vocab in seq_int_]\n",
    "    return seq_ohe\n",
    "\n",
    "seq_ohe = onehot_transform(seq_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1,1, figsize=(9,5))\n",
    "\n",
    "vocab_id = 0\n",
    "\n",
    "ax.imshow(seq_ohe[vocab_id], cmap='bone')\n",
    "ax.set_xlabel('Feature dim')\n",
    "ax.set_yticks([])\n",
    "ax.set_title(f'The one hot representation of \"{seq[vocab_id]}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate training data\n",
    "def gen_data(seq_len): \n",
    "    seq, seq_int = gen_story(all_vocabs, seq_len)\n",
    "    seq_ohe = onehot_transform(seq_int)\n",
    "    # to sequence to pytorch format\n",
    "    seq_ohe_merged = list(itertools.chain(*seq_ohe))\n",
    "    X = np.array(seq_ohe_merged)\n",
    "    return X, seq\n",
    "\n",
    "\n",
    "# how to use `gen_data`\n",
    "seq_len = 25\n",
    "X, seq = gen_data(seq_len)\n",
    "\n",
    "n_time_steps, feature_dim = np.shape(X)\n",
    "inputs = np.vstack([np.zeros(feature_dim), X[:-1,:]])\n",
    "targets = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model params\n",
    "n_input = feature_dim\n",
    "n_hidden = 30\n",
    "n_output = feature_dim\n",
    "\n",
    "# init the layers\n",
    "rnn_in = pnl.TransferMechanism(\n",
    "    name='rnn_in', default_variable=np.zeros((1,n_input))\n",
    ")\n",
    "rnn_hidden = pnl.TransferMechanism(\n",
    "    name='rnn_hidden', default_variable=np.zeros((1,n_hidden)),\n",
    "    function=pnl.Logistic()\n",
    ")\n",
    "rnn_out = pnl.TransferMechanism(\n",
    "    name='rnn_out', default_variable=np.zeros((1,n_output))\n",
    ")\n",
    "\n",
    "# init the weights\n",
    "w_ih = pnl.MappingProjection(\n",
    "    name='input_to_hidden', matrix=np.random.randn(n_input, n_hidden)*0.1,\n",
    "    sender=rnn_in, receiver=rnn_hidden\n",
    ")\n",
    "w_hh = pnl.MappingProjection(\n",
    "    name='hidden_to_hidden', matrix=np.random.randn(n_hidden, n_hidden)*0.1,\n",
    "    sender=rnn_hidden, receiver=rnn_hidden\n",
    ")\n",
    "w_ho = pnl.MappingProjection(\n",
    "    name='hidden_to_output', matrix=np.random.randn(n_hidden, n_output)*0.1,\n",
    "    sender=rnn_hidden, receiver=rnn_out\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = .01\n",
    "pat = 10\n",
    "min_delt = .00001\n",
    "\n",
    "rnn = pnl.AutodiffComposition(\n",
    "    param_init_from_pnl=True,\n",
    "    patience=pat,\n",
    "    min_delta=min_delt,\n",
    "    learning_rate=learning_rate,\n",
    "    optimizer_type='sgd', \n",
    "    randomize=False\n",
    ")\n",
    "\n",
    "# add the mechanisms (add_node) and projections (add_projection) to AutodiffComposition\n",
    "rnn.add_node(rnn_in)\n",
    "rnn.add_node(rnn_hidden)\n",
    "rnn.add_node(rnn_out)\n",
    "\n",
    "rnn.add_projection(\n",
    "    sender=rnn_in, projection=w_ih, receiver=rnn_hidden)\n",
    "rnn.add_projection(\n",
    "    sender=rnn_hidden, projection=w_hh, receiver=rnn_hidden)\n",
    "rnn.add_projection(\n",
    "    sender=rnn_hidden, projection=w_ho, receiver=rnn_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 25\n",
    "num_epochs = 1\n",
    "\n",
    "for i in range(100): \n",
    "    \n",
    "    X, seq = gen_data(seq_len)\n",
    "    n_time_steps, feature_dim = np.shape(X)\n",
    "    inputs = np.vstack([np.zeros(feature_dim), X[:-1,:]])\n",
    "    targets = X    \n",
    "    \n",
    "    input_dict = {\n",
    "        'inputs': {rnn_in: inputs}, \n",
    "        'targets': {rnn_out: targets}, \n",
    "        'epochs': num_epochs\n",
    "    }\n",
    "\n",
    "    # run the model\n",
    "    result = rnn.run(inputs=input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_id = rnn.default_execution_id\n",
    "\n",
    "f, ax = plt.subplots(1,1, figsize = (10,5))\n",
    "ax.plot(rnn.parameters.losses.get(exec_id))\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Average loss')\n",
    "ax.set_title('Learning curve')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "in general, the error function over time peaks right after event(word) boundaries.\n",
    "\"\"\"\n",
    "# calculate error \n",
    "acts = np.squeeze(result)\n",
    "error = [None] * (n_time_steps)\n",
    "for t in range(n_time_steps): \n",
    "    error[t] = np.linalg.norm(X[t,:] - acts[t,:])\n",
    "\n",
    "# plot \n",
    "word_boundaries = np.cumsum([len(vocab) for vocab in seq])-1\n",
    "seq_letters = list(itertools.chain(*seq))\n",
    "seq_len_test = len(seq_letters)\n",
    "\n",
    "f,ax = plt.subplots(1,1, figsize=(16, 5))\n",
    "\n",
    "ax.plot(np.arange(0,seq_len_test,1), error)\n",
    "\n",
    "ax.set_title('Instantaneous prediction error')\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Error')\n",
    "\n",
    "for i, letter in enumerate(seq_letters):\n",
    "    ax.annotate(letter, (i, error[i]), fontsize=14)\n",
    "\n",
    "for wb in word_boundaries: \n",
    "    ax.axvline(wb, color='grey', linestyle='--')\n",
    "    \n",
    "sns.despine()\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
